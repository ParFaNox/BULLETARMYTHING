{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ParFaNox/BULLETARMYTHING/blob/main/Workshop_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iNJ-HPM7jVfs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29e14b42-42fc-41fc-ebd5-11060babab3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Setup environment and Gemini client\n",
        "\n",
        "!pip install faiss-cpu\n",
        "!pip install google-genai\n",
        "!pip install dotenv\n",
        "!pip install numpy\n",
        "\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import faiss\n",
        "from dotenv import load_dotenv\n",
        "from google import genai\n",
        "from IPython.display import Markdown, display"
      ],
      "metadata": {
        "id": "ZoW0o2A2jXD-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "load_dotenv(dotenv_path='env')\n",
        "\n",
        "# Initialize Gemini client\n",
        "client = genai.Client(\n",
        "    api_key=\"INSERT API KEY HERE\",\n",
        "    http_options={\"api_version\": \"v1alpha\"}\n",
        ")"
      ],
      "metadata": {
        "id": "SkS4ETh47S1w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gemini 2.5**\n",
        "Gemini 2.5 Flash is a new multimodal generative ai model from the Gemini family developed by Google DeepMind. It now available as an experimental preview release through the Gemini API in Vertex AI and Vertex AI Studio. The model introduces new features and enhanced core capabilities:\n",
        "\n",
        "**Multimodal Live API**: This new API helps you create real-time vision and audio streaming applications with tool use.\n",
        "\n",
        "**Speed and performance**: Gemini 2.5 Flash is the fastest model in the industry, with a 3x improvement in time to first token (TTFT) over 1.5 Flash.\n",
        "Quality: The model maintains quality comparable to larger models like Gemini 2.0 and GPT-4o.\n",
        "\n",
        "**Improved agentic experiences**: Gemini 2.5 delivers improvements to multimodal understanding, coding, complex instruction following, and function calling.\n",
        "\n",
        "**New Modalities**: Gemini 2.5 introduces native image generation and controllable text-to-speech capabilities, enabling image editing, localized artwork creation, and expressive storytelling.\n",
        "\n",
        "To support the new model, we're also shipping an all new SDK that supports simple migration between the Gemini Developer API and the Gemini API in Vertex AI."
      ],
      "metadata": {
        "id": "0IUdno-n404G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc_text = \"\"\"\n",
        "Join us for “RAG with Gemini,” hosted by Google Developer Groups Purdue!\n",
        "Learn how to build your own Retrieval-Augmented Generation (RAG) system powered by Gemini 2.5, Google's next-generation multimodal AI.\n",
        "In this hands-on session, you'll explore how AI can ground its responses in real documents — essential for creating intelligent knowledge assistants.\n",
        "On October 20, 2025, from 6:30 to 7:30 PM, at WALC 2087\n",
        "Free Gemini credits and a take-home RAG project for the first 120 RSVPs!\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "VB6I_MANjhz9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the example text, questions about the Google Developer Group - Purdue's event cannot be answered correctly without accessing the provided documents. The model's general knowledge of return policies is insufficient. Grounding allows the model to:\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Retrieve relevant information**: The system must first locate the pertinent sections within the provided documents that address the user's question about bike helmet returns.\n",
        "\n",
        "\n",
        "**Process and synthesize information**: After retrieving relevant passages, the model must then understand and synthesize the information to construct an accurate answer.\n",
        "\n",
        "**Generate a grounded response**: Finally, the response needs to be directly derived from the factual content of the documents. This ensures accuracy and avoids hallucinations - generating incorrect or nonsensical information not present in the source documents.\n",
        "\n",
        "---\n",
        "\n",
        "Without grounding, the model is forced to guess or extrapolate from its general knowledge, which can lead to inaccurate or misleading responses. The grounding process makes the model's responses more reliable and trustworthy, especially for domain-specific knowledge like store policies for a store, or research opportunities at Purdue for example."
      ],
      "metadata": {
        "id": "KxfROYX65PEf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Chunking the text\n",
        "# Chunking splits the text into different chunks, allowing only relevant chunks to be passed to the LLM, saving token costs\n",
        "\n",
        "def chunk_text(text, chunk_size=80, overlap=20):\n",
        "    # Split the text into a list of words using whitespace as the delimiter.\n",
        "    # The regular expression \\s+ matches any sequence of one or more whitespace characters\n",
        "    words = re.split(r\"\\s+\", text)\n",
        "\n",
        "    # Create an empty chunks array\n",
        "\n",
        "    # Loop through the words list in steps of (chunk_size - overlap).\n",
        "    # Start new chunk 'overlap' words before the end of the previous chunk.\n",
        "    for i in range():\n",
        "\n",
        "        # Extract a slice of words from index 'i' up to 'i + chunk_size'.\n",
        "        # The slice length ensures each chunk contains approximately 'chunk_size' words.\n",
        "        chunk = \" \".join()\n",
        "\n",
        "        # strip() removes leading/trailing spaces; if the result is non-empty, we keep it.\n",
        "        if chunk.strip():\n",
        "\n",
        "\n",
        "    return\n",
        "\n",
        "CHUNKS = chunk_text()\n",
        "print(f\"Document has been split into {len(CHUNKS)} chunks.\")\n"
      ],
      "metadata": {
        "id": "RMjUvuPwjiN_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating text embeddings\n",
        "# Machines cannot understand human text, hence we must convert the text into vectors of numbers for them to be utilized\n",
        "\n",
        "def embed_texts(texts):\n",
        "    \"\"\"\n",
        "    Embed a list of texts using Gemini's text-embedding-004 model.\n",
        "    \"\"\"\n",
        "    # Call embed_content once with a list of texts\n",
        "\n",
        "\n",
        "    # Each embedding object has .values\n",
        "    embeddings = []\n",
        "\n",
        "    # We need to return a numpy array\n",
        "    return\n",
        "\n",
        "\n",
        "# Generate embeddings for document chunks\n",
        "\n",
        "# Build FAISS index\n",
        "\n",
        "\n",
        "#Get the embedding dimension\n",
        "\n",
        "\n",
        "# Create a FAISS index, a vector database of size embedding_dim\n",
        "\n",
        "\n",
        "# Add all the document embeddings to the index\n",
        "\n",
        "\n",
        "print(f\"FAISS index built with {index.ntotal} chunks ({embedding_dim}-dimensional).\")"
      ],
      "metadata": {
        "id": "tIhqFHEklEoV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieving context\n",
        "\n",
        "def retrieve_context(query, k=3):\n"
      ],
      "metadata": {
        "id": "1ukKLwZHmZet"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What are we giving away for no extra charge?\"\n",
        "\n",
        "\n",
        "# Get the without RAG response from Gemini\n",
        "\n",
        "print(\"Without RAG:\\n\" + response_no_rag.candidates[0].content.parts[0].text)"
      ],
      "metadata": {
        "id": "N7N0aSgBmbga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Asking gemini a question, with RAG\n",
        "\n",
        "context_chunks = retrieve_context(query)\n",
        "\n",
        "def build_prompt(query, context_chunks):\n",
        "    context = \"\\n\\n\".join(context_chunks)\n",
        "    return f\"\"\"\n",
        "You are a helpful assistant, I will be providing you some context text, along with a question.\n",
        "Use the context below to answer the question accurately in proper sentences.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{query}\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Get the with RAG response from Gemini"
      ],
      "metadata": {
        "id": "9sC4uIRrmmLe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Final! Create a full pipeline to get a RAG based answer from Gemini"
      ],
      "metadata": {
        "id": "sa9Hsw3enKGx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}